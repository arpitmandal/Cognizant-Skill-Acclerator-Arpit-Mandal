After fine-tuning GPT-3.5 Turbo using the Bitext dataset in Azure AI Studio, I evaluated the model’s performance using accuracy, BLEU score, and perplexity. Testing with queries like “cancel order 12345” showed improved intent recognition (e.g., cancel_order), achieving ~85% accuracy on a 20% validation split. BLEU scores averaged 0.7, indicating responses closely matched expected outputs, while perplexity dropped, reflecting better fluency. However, challenges emerged: the dataset’s ~100 examples risked overfitting, causing the model to struggle with unseen variations (e.g., “abort purchase 67890”). I mitigated this by limiting epochs to 2 and monitoring Azure’s training loss. Placeholder handling (e.g., {{Order Number}}) occasionally led to inconsistent responses, requiring diverse test inputs. Latency increased slightly post-tuning, critical for customer service, so I optimized batch size to 4. The dataset’s formal tone introduced bias, which I’d address by diversifying future data. Azure’s real-time metrics were instrumental in tracking performance, ensuring the chatbot met reliability goals. Overall, the fine-tuned model enhanced customer support interactions, but scaling the dataset and refining placeholder logic would further improve robustness.